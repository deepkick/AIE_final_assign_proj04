import streamlit as st
import pandas as pd
import openai
import os

st.set_page_config(
    page_title="è¬›ç¾©ã®è³ªç–‘å¿œç­”ã¾ã¨ã‚ã‚¢ãƒ—ãƒª : AIE Proj 04", layout="centered"
)
st.title("ğŸ“š è¬›ç¾©ã®è³ªç–‘å¿œç­”ã¾ã¨ã‚ã‚¢ãƒ—ãƒª : AIE Proj 04")

# Streamlit Cloud Secrets ã‹ã‚‰ OpenAI APIã‚­ãƒ¼ã‚’å–å¾—
openai_api_key = st.secrets["openai_api_key"]

# OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆopenai>=1.0.0å¯¾å¿œï¼‰
client = openai.OpenAI(api_key=openai_api_key)

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
uploaded_file = st.file_uploader(
    "ğŸ“¤ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè³ªå•, å›ç­”ã®åˆ—ã‚’å«ã‚€ï¼‰", type=["csv"]
)

if uploaded_file:
    df = pd.read_csv(uploaded_file)

    if "è³ªå•" not in df.columns:
        st.error("'è³ªå•' ã¨ã„ã†åˆ—ãŒå¿…è¦ã§ã™ã€‚CSVã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    else:
        st.success(f"âœ… {len(df)} ä»¶ã®è³ªå•ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")

        if st.button("â–¶ï¸ GPTã§ä»£è¡¨è³ªå•ã‚’è¦ç´„ã™ã‚‹"):
            prompt = """
ä»¥ä¸‹ã®è³ªå•ã¯è¬›ç¾©ä¸­ã«å—ã‘ãŸä¼¼ãŸå†…å®¹ã®è³ªå•ã§ã™ã€‚ã“ã‚Œã‚‰ã‚’è¦ç´„ã—ã¦ã€ä»£è¡¨çš„ãª1ã¤ã®è³ªå•ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚
è³ªå•ä¸€è¦§ï¼š
"""
            questions = df["è³ªå•"].dropna().tolist()
            prompt += "\n".join(
                [f"- {q}" for q in questions[:20]]
            )  # æœ€åˆã®20ä»¶ã§ãƒ†ã‚¹ãƒˆ
            prompt += "\n\nä»£è¡¨è³ªå•ï¼š"

            with st.spinner("GPTãŒè¦ç´„ä¸­..."):
                try:
                    response = client.chat.completions.create(
                        model="gpt-4",
                        messages=[
                            {
                                "role": "system",
                                "content": "ã‚ãªãŸã¯è¦ªåˆ‡ã§è¦ç´„ãŒä¸Šæ‰‹ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚",
                            },
                            {"role": "user", "content": prompt},
                        ],
                        temperature=0.5,
                    )
                    summary_question = response.choices[0].message.content.strip()
                    st.session_state["summary_question"] = summary_question

                    usage = response.usage
                    if usage:
                        st.session_state["summary_usage"] = usage.total_tokens

                except Exception as e:
                    st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

        # ğŸ’¬ ä»£è¡¨è³ªå•ã®è¡¨ç¤ºï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰ï¼‰
        if "summary_question" in st.session_state:
            st.subheader("ğŸ’¬ ä»£è¡¨è³ªå•ï¼ˆè¦ç´„ï¼‰")
            st.markdown(st.session_state["summary_question"])
            if "summary_usage" in st.session_state:
                st.info(
                    f"ğŸ”¢ ãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»é‡: {st.session_state['summary_usage']} tokens"
                )

            # ğŸ’¡ æ¨¡ç¯„å›ç­”ã®è‡ªå‹•ç”Ÿæˆï¼ˆå¸¸ã«è¡¨ç¤ºã€çŠ¶æ…‹ä¾å­˜ï¼‰
            if st.button("ğŸ’¡ ã“ã®è³ªå•ã«å¯¾ã™ã‚‹æ¨¡ç¯„å›ç­”ã‚’ç”Ÿæˆ"):
                answer_prompt = f"ä»¥ä¸‹ã®è³ªå•ã«å¯¾ã—ã¦ã€è¬›ç¾©ã§ä½¿ãˆã‚‹æ¨¡ç¯„çš„ãªå›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n\nè³ªå•ï¼š{st.session_state['summary_question']}\n\nå›ç­”ï¼š"
                with st.spinner("GPTãŒæ¨¡ç¯„å›ç­”ã‚’ç”Ÿæˆä¸­..."):
                    try:
                        answer_response = client.chat.completions.create(
                            model="gpt-4",
                            messages=[
                                {
                                    "role": "system",
                                    "content": "ã‚ãªãŸã¯è¦ªåˆ‡ã§ã‚ã‹ã‚Šã‚„ã™ãèª¬æ˜ã§ãã‚‹è¬›ç¾©ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚",
                                },
                                {"role": "user", "content": answer_prompt},
                            ],
                            temperature=0.7,
                        )
                        model_answer = answer_response.choices[
                            0
                        ].message.content.strip()
                        st.session_state["model_answer"] = model_answer

                        answer_usage = answer_response.usage
                        if answer_usage:
                            st.session_state["answer_usage"] = answer_usage.total_tokens

                    except Exception as e:
                        st.error(f"æ¨¡ç¯„å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

        # ğŸ“ æ¨¡ç¯„å›ç­”ã®è¡¨ç¤º
        if "model_answer" in st.session_state:
            st.subheader("ğŸ“ æ¨¡ç¯„å›ç­”ï¼ˆGPTç”Ÿæˆï¼‰")
            st.markdown(st.session_state["model_answer"])
            if "answer_usage" in st.session_state:
                st.info(
                    f"ğŸ”¢ ãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»é‡ï¼ˆå›ç­”ç”Ÿæˆï¼‰: {st.session_state['answer_usage']} tokens"
                )
